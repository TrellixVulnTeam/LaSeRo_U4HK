# LaSeRo
## Landsat 8 Segmentation of Rock Outcrops

This is a research framework for geospatial analysis of Landsat 8 imagery. 

### Repository components
The framework consists of two sets of components. The first set of components make up
a data-preprocessing pipeline that is designed to prepare Landsat 8 imagery of Antarctica
for training with a Segnet semantic segmentation model. 

### The pre-processing steps are:
1. Download Landsat 8 scenes using tools from [landsat-utils](https://github.com/developmentseed/landsat-util).
 **This repository is due to be deprecated soon. We are only using a single component of it, so we have included that component directly in this repository in [this directory](https://github.com/selkind/LaSeRo/tree/master/data_preprocessing/landsat_download)**
2. (Optional) Correct the raw DN values and brightness temperature values in each scene to TOA reflectance
and TOA brightness temperature values using the formulae described [here](https://www.usgs.gov/land-resources/nli/landsat/using-usgs-landsat-level-1-data-product)
3. Generate labels for each Landsat scene by converting feature shapefiles into raster layers. 
  - A tool to burn shapefile features into a raster that has the same spatial extent of any downloaded scene. 
  - These rasterized features serve as labels of the Segnet training data.
  - The rasterize function is located in data_preprocessing/utils/raster_tools.py 
  - The label shapefile currently in use is a product created by the British Antarctic Survey. The methodology for creating this layer is described in:
  
  Burton-Johnson, A., Black, M., Fretwell, P. T., and Kaluza-Gilbert, J.: An automated methodology for differentiating rock from snow, clouds and sea in Antarctica from Landsat 8 imagery: a new rock outcrop map and area estimation for the entire Antarctic continent, The Cryosphere, 10, 1665â€“1677, https://doi.org/10.5194/tc-10-1665-2016, 2016. 
  
  - Any work using this layer as a training label must reference the publication above.
  
4. Combine all band TIFs from a scene into a stacked numpy array and break the stack and label layer into chunks of 512 X 512 pixels and save them as pickled .npy files. A function to do this step is in data_preprocessing/utils/raster_tools.py.

#### Once the scenes are stacked and chunked into .npy files, they are ready for model training.

### The training steps are:
1. Generate a model data text file that contains all paths to the chunks that will be used for training. This text file is generated by iterating over the stacked chunk and label directory and saving a stack, label pair on each line separated by a space (" "). **This should be refactored to csv or JSON soon for easier file creation/modification**
  - Two model data text files need to be generated for each model iteration. One for training chunks, one for validation chunks.
  - Examples of these data files are available in the metadata/ directory.
  - The last cell in colab_notebooks/model_SegNet_bands_all_data_1percent.ipynb contains a function that can be used to generate a data text file.
2. Configure the model input shape and hyper-parameters in the configuration cell of the model notebook.
  - The model configuration that yielded the best results so far is in colab_notebooks/model_SegNet_bands_all_data_1percent.ipynb
  - The key parameters that must set correctly and consistently for the model to run are the set_info, image_shape, training_set_size, and val_set_size.
  - The paths to the training and validation data, as well as the saved weights are saved in the first 4 paths. test_set_path is not necessary because we have a custom model testing procedure due to our limited ground-truth data.
3. Train the model
  -after 11 epochs, model weights are saved every 5 epochs. This is to mitigate wasted training time due to Google Colab frequently disconnecting runtimes.
  - After every epoch, the validation loss is checked against the minimum validation loss of the training session. If it is lower than the current minimum, the weights of that epoch are saved as a "best-case" model.
  - Model weights are saved according to a specific schema: "model_{bandcombination}_{epoch#}\_lr\_{learningratevalue}". ex: "model_allBands_epoch40_lr_0.01"
  
  #### A trained model must be evaluated for accuracy by comparing outputs to the ground truth layers created and provided by Burton-Johnson et. al., 2016
  
### The model evaluation steps are:
1. Generate model output on all scenes listed in metadata/qc_ids.txt using the predictor notebook in colab_notebooks/predictor.ipynb. The several variables defined in the configuration cell of this notebook must match the variables as they were defined in the model training notebook. (these change for different models). The specific vars are: 
  - best_model (this is the name of the model you want to generate output for)
  - num_classes
  - image_shape
  - set_info
  - Also note that test_set_size must be 1709 (the number of chunks in the scenes listed in qc_ids.txt.
  2. Use the predictor notebook to save the output of the model as binary rasters in the output directory.
  3. Use the predictor notebook to generate statistics on the output by comparing the output to 10 manually labeled areas (Created by Burton-Johnson et. al., 2016.
  4. Save the generated statistics as JSON files in the results directory.
  5. Use the notebook colab_notebooks/model_assessment.ipynb to visualize the output from the predictor notebook. These include visualizations of:
  - The input band data
  - the band combinations used to generate the labels (methodology by Burton-Johnson et. al., 2016)
  - differences between the model output and the manual labels overlaid on a full color image of the area for qualitative analysis.
  6. Use the model_assessment notebook to plot each scene in terms of an accuracy metric (e.g. rock commission error) vs. an image condition (e.g. sun elevation at time of scene capture).


## Programming Environment Description
Currently two different computing environments are needed. One environment for the data acquisition and pre-processing steps, and Google Colab for the model training, scene prediction output, and model evaluation. It is likely that the data acquisition and pre-processing steps will work on Google Colab, but it has not yet been tested. The current workflow for this framework is as follows:
1. Download and pre-process Landsat 8 scenes in Google Cloud or on a local computer with enough storage capacity for your selected scenes.
- A single scene is ~1GB if all bands are included. The corrected bands will take up another ~1GB and the stacked chunks and labels will take up another ~1GB. (roughly 3GB/ scene once all pre-processing steps have been completed).
2. Transfer your stacked chunks and labels to a Google Drive so they can be mounted to Google Colab machines. This is the best way to deal with data IO when working in Google Colab.
3. Train, evaluate, and analyze models in Google Colab.


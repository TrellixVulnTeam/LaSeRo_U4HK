{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euVM0pt-w0Xs"
   },
   "outputs": [],
   "source": [
    "#!tar -xzvf  /content/drive/My\\ Drive/LC82121112013339LGN00.tar\n",
    "training_set_path='/content/drive/My Drive/Metadata/train.txt'\n",
    "val_set_path='/content/drive/My Drive/Metadata/val.txt'\n",
    "test_set_path='/home/dsa/DSA/qc/test.txt'\n",
    "model_path='/home/dsa/DSA/qc/models/'\n",
    "\n",
    "num_classes=2\n",
    "image_shape=(512,512,4)\n",
    "padding=((0,0),(0,0))\n",
    "batch_size=2\n",
    "epochs=10\n",
    "\n",
    "train_subset_size = 2000\n",
    "val_subset_size = 500\n",
    "n_splits = 3\n",
    "set_info={0:[1,2,3,7] #2,3,5,6,10-important bands \n",
    "          }\n",
    "\n",
    "training_set_size=2000\n",
    "val_set_size=500\n",
    "test_set_size=2\n",
    "optimizer='sgd'\n",
    "loss_function='categorical_crossentropy'\n",
    "metrics=['accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yP4Xqz1UzR2_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/dsa/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dsa/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dsa/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dsa/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dsa/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dsa/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/dsa/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dsa/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dsa/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dsa/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dsa/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dsa/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation,BatchNormalization,Conv2D\n",
    "from keras.engine.topology import Layer\n",
    "#import keras.backend as K\n",
    "#import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "class MaxPoolingWithIndices(Layer):\n",
    "    def __init__(self, pool_size,strides,padding='SAME',**kwargs):\n",
    "        super(MaxPoolingWithIndices, self).__init__(**kwargs)\n",
    "        self.pool_size=pool_size\n",
    "        self.strides=strides\n",
    "        self.padding=padding\n",
    "        return\n",
    "    def call(self,x):\n",
    "        pool_size=self.pool_size\n",
    "        strides=self.strides\n",
    "        if isinstance(pool_size,int):\n",
    "            ps=[1,pool_size,pool_size,1]\n",
    "        else:\n",
    "            ps=[1,pool_size[0],pool_size[1],1]\n",
    "        if isinstance(strides,int):\n",
    "            st=[1,strides,strides,1]\n",
    "        else:\n",
    "            st=[1,strides[0],strides[1],1]\n",
    "        output1,output2=K.tf.nn.max_pool_with_argmax(x,ps,st,self.padding)\n",
    "        return [output1,output2]\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if isinstance(self.pool_size,int):\n",
    "            output_shape=(input_shape[0],input_shape[1]//self.pool_size,input_shape[2]//self.pool_size,input_shape[3])\n",
    "        else:\n",
    "            output_shape=(input_shape[0],input_shape[1]//self.pool_size[0],input_shape[2]//self.pool_size[1],input_shape[3])\n",
    "        return [output_shape,output_shape]\n",
    "\n",
    "\n",
    "class UpSamplingWithIndices(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(UpSamplingWithIndices, self).__init__(**kwargs)\n",
    "        return\n",
    "    def call(self,x):\n",
    "        argmax=K.cast(K.flatten(x[1]),'int32')\n",
    "        max_value=K.flatten(x[0])\n",
    "        with K.tf.variable_scope(self.name):\n",
    "            input_shape=K.shape(x[0])\n",
    "            batch_size=input_shape[0]\n",
    "            image_size=input_shape[1]*input_shape[2]*input_shape[3]\n",
    "            output_shape=[input_shape[0],input_shape[1]*2,input_shape[2]*2,input_shape[3]]\n",
    "            indices_0=K.flatten(K.tf.matmul(K.reshape(K.tf.range(batch_size),(batch_size,1)),K.ones((1,image_size),dtype='int32')))\n",
    "            indices_1=argmax%(image_size*4)//(output_shape[2]*output_shape[3])\n",
    "            indices_2=argmax%(output_shape[2]*output_shape[3])//output_shape[3]\n",
    "            indices_3=argmax%output_shape[3]\n",
    "            indices=K.tf.stack([indices_0,indices_1,indices_2,indices_3])\n",
    "            output=K.tf.scatter_nd(K.transpose(indices),max_value,output_shape)\n",
    "            return output\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0][0],input_shape[0][1]*2,input_shape[0][2]*2,input_shape[0][3]\n",
    "\n",
    "def CompositeConv(inputs,num_layers,num_features):\n",
    "    output=inputs\n",
    "    if isinstance(num_features,int):\n",
    "        for i in range(num_layers):\n",
    "            output=Conv2D(num_features,(7,7),padding='same')(output)\n",
    "            output=BatchNormalization(axis=3)(output)\n",
    "            output=Activation('relu')(output)\n",
    "        return output\n",
    "    for i in range(num_layers):\n",
    "        output=Conv2D(num_features[i],(7,7),padding='same')(output)\n",
    "        output=BatchNormalization(axis=3)(output)\n",
    "        output=Activation('relu')(output)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BXX_mRWz1gDQ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Activation,Input,ZeroPadding2D,Cropping2D\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    inputs=Input(shape=image_shape)\n",
    "\n",
    "    x = ZeroPadding2D(padding)(inputs)\n",
    "\n",
    "    x=CompositeConv(x,2,64)\n",
    "    x,argmax1=MaxPoolingWithIndices(pool_size=2,strides=2)(x)\n",
    "    \n",
    "    x=CompositeConv(x,2,64)\n",
    "    x,argmax2=MaxPoolingWithIndices(pool_size=2,strides=2)(x)\n",
    "    \n",
    "    x=CompositeConv(x,3,64)\n",
    "    x,argmax3=MaxPoolingWithIndices(pool_size=2,strides=2)(x)\n",
    "\n",
    "    x=CompositeConv(x,3,64)\n",
    "    x,argmax4=MaxPoolingWithIndices(pool_size=2,strides=2)(x)\n",
    "\n",
    "    x=CompositeConv(x,3,64)\n",
    "    x,argmax5=MaxPoolingWithIndices(pool_size=2,strides=2)(x)\n",
    "\n",
    "    x=UpSamplingWithIndices()([x,argmax5])\n",
    "    x=CompositeConv(x,3,64)\n",
    "\n",
    "    x=UpSamplingWithIndices()([x,argmax4])\n",
    "    x=CompositeConv(x,3,64)\n",
    "\n",
    "    x=UpSamplingWithIndices()([x,argmax3])\n",
    "    x=CompositeConv(x,3,64)\n",
    "\n",
    "    x=UpSamplingWithIndices()([x,argmax2])\n",
    "    x=CompositeConv(x,2,64)\n",
    "    \n",
    "    x=UpSamplingWithIndices()([x,argmax1])\n",
    "    x=CompositeConv(x,2,[64,num_classes])\n",
    "\n",
    "    x=Activation('softmax')(x)\n",
    "\n",
    "    y=Cropping2D(padding)(x)\n",
    "    my_model=Model(inputs=inputs,outputs=y)\n",
    "    \n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XmrvXys1i8p"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def read_image_batch(image_list, batch_size, channel_list):\n",
    "    while True:\n",
    "        l=len(image_list)\n",
    "        num_batch=l//batch_size\n",
    "        if num_batch*batch_size<l:\n",
    "            num_batch+=1\n",
    "        for i in range(num_batch):\n",
    "           \n",
    "            batch_set=image_list[batch_size*i:min(batch_size*(i+1),l),:]\n",
    "            batch_set=[batch_set[bs] for bs in range(len(batch_set))]\n",
    "            X=np.array([np.load(line[0][0:]) for line in batch_set])\n",
    "            labels=np.array([np.load(line[1][0:]) for line in batch_set])\n",
    "            y=to_categorical(labels,num_classes)\n",
    "            X = X[:, :, :, channel_list]\n",
    "            yield tuple((X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SpryaCe8Al5f"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-311c809bf64f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstr_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtest_image_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtest_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test_names=[]\n",
    "\n",
    "with open(test_set_path,\"r\") as f:\n",
    "  test_image_list=[]\n",
    "  for line in f.readlines():\n",
    "    arr=[]\n",
    "    str_array=line.split(\" \")\n",
    "    arr.append(str_array[0]+\" \"+str_array[1])\n",
    "    arr.append(str_array[2]+\" \"+str_array[3][:-1])\n",
    "    test_image_list.append(arr)\n",
    "    test_names.append(line)\n",
    "\n",
    "test_image_list=np.asarray(test_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "my-pI2L4smKf"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "'''\n",
    "def read_test_image_batch(data_path,batch_size,channel):\n",
    "  image_list=[]\n",
    "  with open(data_path,\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "      arr=[]\n",
    "      str_array=line.split(\" \")\n",
    "      arr.append(str_array[0]+\" \"+str_array[1])\n",
    "      arr.append(str_array[2]+\" \"+str_array[3][:-1])\n",
    "      image_list.append(arr)\n",
    "  image_list=np.asarray(image_list)\n",
    " \n",
    "  \n",
    "  while True: \n",
    "    l=len(image_list)\n",
    "    num_batch=l//batch_size\n",
    "    if num_batch*batch_size<l:\n",
    "        num_batch+=1\n",
    "       \n",
    "    for i in range(num_batch):\n",
    "      batch_set=image_list[batch_size*i:min(batch_size*(i+1),l),:]\n",
    "      batch_set=[batch_set[bs] for bs in range(len(batch_set))]\n",
    "      X=np.array([np.load(line[0][0:]) for line in batch_set])\n",
    "      X=X[:,:,:,channel]\n",
    "      labels=np.array([np.load(line[1][0:]) for line in batch_set])\n",
    "      y=to_categorical(labels,num_classes)\n",
    "      \n",
    "      yield tuple((X, y))\n",
    "\n",
    "'''\n",
    "\n",
    "def read_test_image_batch(image_list, batch_size, channel_list):\n",
    "    while True:\n",
    "        l=len(image_list)\n",
    "        num_batch=l//batch_size\n",
    "        if num_batch*batch_size<l:\n",
    "            num_batch+=1\n",
    "        for i in range(num_batch):\n",
    "            batch_set=image_list[batch_size*i:min(batch_size*(i+1),l),:]\n",
    "            batch_set=[batch_set[bs] for bs in range(len(batch_set))]\n",
    "            X=np.array([np.load(line[0][0:]) for line in batch_set])\n",
    "            labels=np.array([np.load(line[1][0:]) for line in batch_set])\n",
    "            y=to_categorical(labels,num_classes)\n",
    "            X = X[:, :, :, channel_list]\n",
    "            yield tuple((X, y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55indsfD1s5J"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation,Input\n",
    "\n",
    "\n",
    "\n",
    "def getSubsets(data_path, set_info, subset_size):\n",
    "  image_list=[]\n",
    "  with open(data_path,\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "      arr=[]\n",
    "      str_array=line.split(\" \")\n",
    "      arr.append(str_array[0]+\" \"+str_array[1])\n",
    "      arr.append(str_array[2]+\" \"+str_array[3][:-1])\n",
    "      image_list.append(arr)\n",
    "\n",
    "  #getSubsets(\"/content/drive/My Drive/Metadata/test.txt\",set_info,)\n",
    "     \n",
    "  image_list = np.asarray(image_list)\n",
    "  np.random.seed(1)\n",
    "  np.random.shuffle(image_list)\n",
    "  data = []\n",
    "  jump = int(subset_size - ((subset_size*n_splits)-image_list.shape[0])/2)\n",
    "  for i in range(0, image_list.shape[0]-subset_size, jump):\n",
    "      #arr = np.random.randint(0, len(image_list), subset_size)\n",
    "      #subset = image_list[arr,]\n",
    "      subset = image_list[i: i+subset_size,]\n",
    "      for key, value in set_info.items():        \n",
    "          data.append(read_image_batch(subset, batch_size, value))\n",
    "           \n",
    "  return np.asarray(data)\n",
    "            \n",
    "\n",
    "def main(args):\n",
    "    #make_segnet_input_file('/content/drive/My Drive',['LC82121112013339LGN00'],'/content/drive/My Drive/Metadata/train.txt')\n",
    "      \n",
    "    if args['resume']:\n",
    "        my_model.load_weights(model_path+args['load'])\n",
    "\n",
    "    \n",
    "    train_data = getSubsets(training_set_path, set_info, train_subset_size)\n",
    "    val_data = getSubsets(val_set_path, set_info, val_subset_size)\n",
    "  \n",
    "    key=0\n",
    "    for i in range(1, len(train_data)):\n",
    "      my_model=create_model()\n",
    "      my_model.compile(optimizer,loss=loss_function,metrics=metrics)\n",
    "      min_loss = 1.0\n",
    "      for j in range(epochs):\n",
    "        hist = my_model.fit_generator(train_data[i],\n",
    "                                steps_per_epoch=(train_subset_size+1)//batch_size,\n",
    "                                epochs=1,validation_data=val_data[i],\n",
    "                                validation_steps=(val_subset_size+1)//batch_size)\n",
    "        val_loss = hist.history[\"val_loss\"]\n",
    "        print(val_loss)\n",
    "        if val_loss[0] < min_loss:\n",
    "          min_loss = val_loss[0]\n",
    "          my_model.save_weights(model_path+args['save']+\"_\"+str(i)+\"_key_\"+str(key))\n",
    "        print(\"model fit epoch: \"+str(j))\n",
    "        #my_model.save_weights(model_path+args['save']+\"_\"+str(i)+\"_key_\"+str(key))\n",
    "      \n",
    "      \n",
    "    \n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--save\",default='my_model')\n",
    "#parser.add_argument(\"--resume\",action='store_true')\n",
    "#parser.add_argument(\"--load\",default='my_model')\n",
    "args = {\"save\": \"model_A\", \"resume\":False, \"load\": \"model_A\"}\n",
    "\n",
    "# main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = read_image_batch(test_set_path, 2, set_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 343267,
     "status": "ok",
     "timestamp": 1573590595302,
     "user": {
      "displayName": "whereare therocks",
      "photoUrl": "",
      "userId": "11641855154344275890"
     },
     "user_tz": 300
    },
    "id": "BW4Mjj1a-mY6",
    "outputId": "4d2ebd35-29df-4035-efd5-85c7d207472d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dsa/DSA/qc/models/model_A_0_key_0\n",
      "model created in 61.096346378326416 seconds\n",
      "model compiled in 62.69204378128052 seconds\n",
      "model weights loaded in 72.31946659088135 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7cf46adf9582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m#print(my_model.evaluate_generator(test_data,steps=(test_set_size+1)//batch_size))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m#print(my_model.metrics_names)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mtoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predictions generated in {} seconds\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1431\u001b[0m                 \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m                 use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1846\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m   1847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(model, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                 \u001b[0;31m# Compatibility with the generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project/AntarcticRockOutcrop/antrock/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation,Input\n",
    "from scipy.stats import mode\n",
    "import os\n",
    "import time\n",
    "#from custom_layers import MaxPoolingWithIndices,UpSamplingWithIndices,CompositeConv\n",
    "#import config as cf\n",
    "#from util import read_image_batch\n",
    "#from create_model import create_model\n",
    "\n",
    "class_labels=[]\n",
    "\n",
    "\n",
    "for i in os.listdir(model_path):\n",
    "    if(i==\".ipynb_checkpoints\"):\n",
    "        continue\n",
    "\n",
    "    key=int(i[-1:])\n",
    "    if(key!=0):\n",
    "        continue\n",
    "\n",
    "    print(model_path+i)\n",
    "    tic = time.time()\n",
    "    \n",
    "    my_model=create_model()\n",
    "    toc = time.time() - tic\n",
    "    print(\"model created in {:.2f} seconds\".format(toc))\n",
    "    \n",
    "    my_model.compile(optimizer,loss=loss_function,metrics=metrics)\n",
    "    toc = time.time() - tic\n",
    "    print(\"model compiled in {:.2f} seconds\".format(toc))\n",
    "    \n",
    "    my_model.load_weights(model_path+i)\n",
    "    toc = time.time() - tic\n",
    "    print(\"model weights loaded in {:.2f} seconds\".format(toc))\n",
    "    \n",
    "    test_data=read_test_image_batch(test_image_list,batch_size,set_info[key])#(test_set_path,batch_size,set_info[key])\n",
    "    #print(my_model.evaluate_generator(test_data,steps=(test_set_size+1)//batch_size))\n",
    "    #print(my_model.metrics_names)\n",
    "    probs=my_model.predict(test_data,steps=(test_set_size+1)//batch_size)\n",
    "    toc = time.time() - tic\n",
    "    print(\"predictions generated in {:.2f} seconds\".format(toc))\n",
    "    print(\"\\nprobs\")\n",
    "    print(probs.shape)\n",
    "    class_labels.append(probs.argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels=np.asarray(class_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "u,indices=np.unique(class_labels,return_inverse=True)\n",
    "final_labels=u[np.argmax(np.apply_along_axis(np.bincount,0,indices.reshape(class_labels.shape),None,np.max(indices)+1),axis=0)]\n",
    "\n",
    "for i in range(test_set_size):\n",
    "  line=test_names[i]\n",
    "  print(line)\n",
    "  str_array=line.split(\" \")\n",
    "  scene_id=str_array[1][18:40]\n",
    "  out_str=str_array[1][40:-4]+\"_output.npy\"\n",
    "  print(out_str)\n",
    "#   np.save('/content/drive/My Drive/new_test_outputs_key_0/'+scene_id+out_str,final_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JqdvLZbJJ5Qi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This script takes a list of scene ids and creates a file that can be used as input for a segnet model\n",
    "@param string chunk_dir: The abspath base directory where each set of chunks for a scene has its own dir named with its sceneID\n",
    "@param list scene_ids: A list of sceneIDs that exist in the chunk_dir. The chunks of these scenes will be used in the file.\n",
    "@param string out_path: The abspath where the resulting file should be saved.\n",
    "@return int lines_written: the total number of lines (corresponding to a data and label chunk path) in the file.\n",
    "file format:\n",
    "/path/to/scene_chunk.npy /path/to/scene_chunk_label.npy\n",
    "/path/to/scene_chunk.npy /path/to/scene_chunk_label.npy\n",
    "/path/to/scene_chunk.npy /path/to/scene_chunk_label.npy\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "def make_segnet_input_file(chunk_dir, scene_ids, out_path):\n",
    "    existing_scenes = [i for i in os.listdir(chunk_dir) if os.path.isdir(os.path.join(chunk_dir, i))]\n",
    "    # filter out ids that don't exist in the given dir\n",
    "    scene_ids = [i for i in scene_ids if i in existing_scenes]\n",
    "    print(scene_ids)\n",
    "\n",
    "    lines_to_write = []\n",
    "\n",
    "    for i in scene_ids:\n",
    "        scene_dir = os.path.join(chunk_dir, i)\n",
    "        for j in os.listdir(scene_dir):\n",
    "            if j[-9:] != \"label.npy\":\n",
    "                data_path = os.path.join(scene_dir, j)\n",
    "                file_split = os.path.splitext(j)\n",
    "                label_path = os.path.join(scene_dir, file_split[0] + \"_label\" + file_split[1])\n",
    "\n",
    "                lines_to_write.append(\"{} {}\\n\".format(data_path, label_path))\n",
    "\n",
    "    with open(out_path, 'w+') as output:\n",
    "        output.writelines(lines_to_write)\n",
    "\n",
    "    return len(lines_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LC82201072015017LGN00']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_dir = \"/home/dsa/DSA/qc/stacked_chunks\"\n",
    "scene_id = [\"LC82201072015017LGN00\"]\n",
    "# existing scenes = [i for i in os.listdir(chunk_dir) if os.path.isdir(os.path.join(chunk_dir, i))]\n",
    "# scene_ids = \n",
    "make_segnet_input_file(chunk_dir, scene_id, test_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59258,
     "status": "ok",
     "timestamp": 1573589047494,
     "user": {
      "displayName": "whereare therocks",
      "photoUrl": "",
      "userId": "11641855154344275890"
     },
     "user_tz": 300
    },
    "id": "AKJ0WYEK39xn",
    "outputId": "cf0c941b-4c5d-4d37-e851-7bda105aabed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ntm1iFciogKi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAs1RsQLVfSr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SegNetCode_A.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

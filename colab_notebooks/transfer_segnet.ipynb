{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transfer_segnet.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BZ80yyThVsON","colab_type":"text"},"source":["#Configuration Parameters"]},{"cell_type":"code","metadata":{"id":"AKJ0WYEK39xn","colab_type":"code","outputId":"c45576e9-8895-483e-9114-d7bcd4da00cf","executionInfo":{"status":"ok","timestamp":1589588072725,"user_tz":240,"elapsed":176415,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":129}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qWRDYCBmLAOq","colab_type":"code","outputId":"60748395-0ff8-4cbb-a724-5006e8552e44","executionInfo":{"status":"ok","timestamp":1589588272778,"user_tz":240,"elapsed":7108,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import sys\n","import os\n","import argparse\n","import csv\n","\n","import imageio\n","import numpy as np\n","\n","from keras.utils import multi_gpu_model, to_categorical\n","from keras.layers import Activation,Input\n","from keras.models import Model\n","from keras.applications.vgg16 import VGG16\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n","from tensorflow import convert_to_tensor\n","\n","sys.path.append(\"/content/drive/My Drive/tf-keras-SegNet\")\n","from model import segnet"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"n4k2qGbmHZ47","colab_type":"code","colab":{}},"source":["base_dir = \"/content/drive/My Drive/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"anRxznYgHeAV","colab_type":"code","colab":{}},"source":["def create_session_paths(session_name, overwrite=False, base_dir='/content/drive/My Drive/'):\n","    models_dir = os.path.join(base_dir, \"models\")\n","    session_dir = os.path.join(models_dir, session_name)\n","    # Prevent accidental overwriting of previous sessions\n","    try:\n","        os.mkdir(session_dir)\n","    except FileExistsError:\n","        if not overwrite:\n","            print(\"Set overwrite to True if you wish to continue\")\n","            raise FileExistsError\n","        print(\"overwriting session\")\n","\n","    model = os.path.join(session_dir, \"model.h5\")\n","    history = os.path.join(session_dir, \"history.json\")\n","    training_log = os.path.join(session_dir, \"logs.csv\")\n","    training_config = os.path.join(session_dir, \"config.json\")\n","    classification_report = os.path.join(session_dir, \"classification_report.txt\")\n","    return {\"model\": model,\n","            \"history\": history,\n","            \"logs\": training_log,\n","            \"config\": training_config,\n","            \"classification_report\": classification_report}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oy2LMxNpHslq","colab_type":"code","colab":{}},"source":["session_name = \"one_percent_transfer_rgb\"\n","session_paths = create_session_paths(session_name, overwrite=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VFdNPyi-H98O","colab_type":"code","outputId":"121fb219-7096-49f0-ccfd-fe48a9891b5c","executionInfo":{"status":"ok","timestamp":1589588272789,"user_tz":240,"elapsed":5957,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":109}},"source":["for i in session_paths:\n","    print(session_paths[i])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/models/one_percent_transfer_rgb/model.h5\n","/content/drive/My Drive/models/one_percent_transfer_rgb/history.json\n","/content/drive/My Drive/models/one_percent_transfer_rgb/logs.csv\n","/content/drive/My Drive/models/one_percent_transfer_rgb/config.json\n","/content/drive/My Drive/models/one_percent_transfer_rgb/classification_report.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M9idtsPKSejQ","colab_type":"code","colab":{}},"source":["def get_image_list(metadata_file_path):\n","    with open(metadata_file_path, 'r') as f:\n","        return [i for i in csv.reader(f) if i]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"euVM0pt-w0Xs","colab_type":"code","colab":{}},"source":["training_set_path = '/content/drive/My Drive/Metadata/one_percent_train.csv'\n","val_set_path = '/content/drive/My Drive/Metadata/one_percent_val.csv'\n","test_set_path = '/content/drive/My Drive/Metadata/one_percent_test.csv'\n","model_path = session_paths[\"model\"]\n","\n","n_splits = 1\n","bands = [1, 2, 3] # number of bands\n","\n","num_classes = 2\n","image_shape = (512, 512, len(bands))\n","padding = ((0, 0), (0, 0))\n","batch_size = 5\n","epochs = 50\n","learning_rate = 0.02\n","\n","training_set_list = get_image_list(training_set_path)\n","val_set_list = get_image_list(val_set_path)\n","\n","training_set_size = len(training_set_list)\n","val_set_size = len(val_set_list)\n","\n","loss_function = 'categorical_crossentropy'\n","metrics = ['accuracy']\n","callback_metric = \"val_accuracy\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mCwSixkLDTkN","colab_type":"text"},"source":["# Load segnet and vgg model\n"]},{"cell_type":"code","metadata":{"id":"IR10DGtCLo10","colab_type":"code","outputId":"fefbb760-2e13-458d-a2f7-656bf6e0bf6b","executionInfo":{"status":"ok","timestamp":1589590566327,"user_tz":240,"elapsed":3232,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["segnet_model = segnet(image_shape, num_classes)\n","vgg_model = VGG16(include_top=False, weights='imagenet', input_shape=image_shape, classes=num_classes)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Build enceder done..\n","Build decoder done..\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qtQn_aCnaDsB","colab_type":"code","outputId":"5c0e578b-d621-404e-fd7d-8ac2a44cab6c","executionInfo":{"status":"ok","timestamp":1589590566329,"user_tz":240,"elapsed":2706,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["seg_layer_names = [i for i in segnet_model.layers if \"conv\" in i.name]\n","vgg_layer_names = [i for i in vgg_model.layers if \"conv\" in i.name]\n","\n","transferable_layer_names = {}\n","for i in range(len(vgg_layer_names)):\n","    transferable_layer_names[seg_layer_names[i].name] = vgg_layer_names[i]\n","\n","layer_count = 0\n","for i in segnet_model.layers:\n","    try:\n","        i.set_weights(transferable_layer_names[i.name].get_weights())\n","        layer_count += 1\n","    except KeyError:\n","        pass\n","\n","print(layer_count)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["13\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jS01ZdNVDXjJ","colab_type":"text"},"source":["# Transfer weights of matching layers from image-net trained vgg16 to segnet"]},{"cell_type":"markdown","metadata":{"id":"h2NZIihQWBxg","colab_type":"text"},"source":["#Training model \n"]},{"cell_type":"code","metadata":{"id":"55indsfD1s5J","colab_type":"code","colab":{}},"source":["def data_gen(metadata_file_path, bands, batch_size):\n","    image_list = np.asarray(get_image_list(metadata_file_path))\n","    np.random.seed(1)\n","    np.random.shuffle(image_list)\n","\n","    band_normalization_map = []\n","    counter = 0\n","\n","    total_steps = image_list.shape[0] // batch_size\n","    while True:\n","        step_start = counter * batch_size\n","        step_end = step_start + batch_size\n","        images = []\n","        masks = []\n","        for j in range(step_start, step_end):\n","            images.append(np.load(image_list[j, 0])[:,:,bands])\n","            masks.append(np.load(image_list[j, 1]))\n","\n","        y = to_categorical(np.array(masks))\n","        yield np.array(images) / 65535, y.reshape((batch_size, y.shape[1] * y.shape[2], y.shape[3]))\n","\n","        counter +=1\n","\n","        if counter >= total_steps:\n","            counter = 0\n","            np.random.shuffle(image_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SMKEnOm3E8f7","colab_type":"code","outputId":"4dae554d-6b82-490e-ad0d-9021337f6322","executionInfo":{"status":"ok","timestamp":1589596257380,"user_tz":240,"elapsed":5692344,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train_data = data_gen(training_set_path, bands, batch_size)\n","val_data = data_gen(val_set_path, bands, batch_size)\n","\n","segnet_model.compile(optimizer=Adam(learning_rate=learning_rate), loss=loss_function, metrics=metrics)\n","\n","save_model_path, model_file = os.path.split(session_paths[\"model\"])\n","save_model_file, model_ext = os.path.splitext(model_file)\n","save_model_prefix = os.path.join(save_model_path, save_model_file)\n","\n","best_checkpoint = ModelCheckpoint(session_paths[\"model\"],\n","                             monitor=callback_metric,\n","                             verbose=1,\n","                             save_best_only=True,\n","                             mode='max')\n","\n","checkpoint = ModelCheckpoint(save_model_prefix + \"_epoch{epoch:02d}.h5\",\n","                             period=5,\n","                             save_weights_only=False,\n","                             save_best_only=False)\n","\n","reduce_lr = ReduceLROnPlateau(monitor=callback_metric,\n","                              factor=0.5,\n","                              patience=3,\n","                              verbose=1,\n","                              mode='max',\n","                              min_lr=0.0001)\n","\n","csv_logger = CSVLogger(session_paths[\"logs\"])\n","\n","early_stopper = EarlyStopping(monitor=callback_metric,\n","                              patience=9,\n","                              verbose=1,\n","                              mode='max')\n","\n","callbacks_list = [checkpoint, best_checkpoint, reduce_lr, csv_logger, early_stopper]\n","\n","try:\n","    model = multi_gpu_model(model)\n","except:\n","    print(\"single GPU in use\")\n","\n","hist = segnet_model.fit(train_data,\n","                        steps_per_epoch=training_set_size // batch_size,\n","                        epochs=epochs,\n","                        validation_data=val_data,\n","                        validation_steps=val_set_size // batch_size,\n","                        verbose=1,\n","                        callbacks=callbacks_list)\n","\n","val_loss = hist.history[\"val_loss\"]"],"execution_count":17,"outputs":[{"output_type":"stream","text":["single GPU in use\n","Epoch 1/50\n","154/154 [==============================] - 216s 1s/step - loss: 0.3035 - accuracy: 0.9208 - val_loss: 0.4176 - val_accuracy: 0.9199\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.91992, saving model to /content/drive/My Drive/models/one_percent_transfer_rgb/model.h5\n","Epoch 2/50\n","154/154 [==============================] - 188s 1s/step - loss: 0.1944 - accuracy: 0.9281 - val_loss: 0.7306 - val_accuracy: 0.9200\n","\n","Epoch 00002: val_accuracy improved from 0.91992 to 0.92003, saving model to /content/drive/My Drive/models/one_percent_transfer_rgb/model.h5\n","Epoch 3/50\n","154/154 [==============================] - 188s 1s/step - loss: 0.1817 - accuracy: 0.9282 - val_loss: 0.2750 - val_accuracy: 0.9200\n","\n","Epoch 00003: val_accuracy did not improve from 0.92003\n","Epoch 4/50\n","154/154 [==============================] - 188s 1s/step - loss: 0.1662 - accuracy: 0.9294 - val_loss: 0.5176 - val_accuracy: 0.9263\n","\n","Epoch 00004: val_accuracy improved from 0.92003 to 0.92627, saving model to /content/drive/My Drive/models/one_percent_transfer_rgb/model.h5\n","Epoch 5/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1656 - accuracy: 0.9290 - val_loss: 0.1876 - val_accuracy: 0.9255\n","\n","Epoch 00005: val_accuracy did not improve from 0.92627\n","Epoch 6/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1648 - accuracy: 0.9295 - val_loss: 0.1621 - val_accuracy: 0.9189\n","\n","Epoch 00006: val_accuracy did not improve from 0.92627\n","Epoch 7/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1640 - accuracy: 0.9302 - val_loss: 0.0796 - val_accuracy: 0.9214\n","\n","Epoch 00007: val_accuracy did not improve from 0.92627\n","\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.009999999776482582.\n","Epoch 8/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1582 - accuracy: 0.9311 - val_loss: 0.1703 - val_accuracy: 0.9198\n","\n","Epoch 00008: val_accuracy did not improve from 0.92627\n","Epoch 9/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1576 - accuracy: 0.9315 - val_loss: 0.2550 - val_accuracy: 0.8989\n","\n","Epoch 00009: val_accuracy did not improve from 0.92627\n","Epoch 10/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1565 - accuracy: 0.9324 - val_loss: 0.3859 - val_accuracy: 0.9195\n","\n","Epoch 00010: val_accuracy did not improve from 0.92627\n","\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n","Epoch 11/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1567 - accuracy: 0.9328 - val_loss: 0.1707 - val_accuracy: 0.9187\n","\n","Epoch 00011: val_accuracy did not improve from 0.92627\n","Epoch 12/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1599 - accuracy: 0.9320 - val_loss: 0.1815 - val_accuracy: 0.9206\n","\n","Epoch 00012: val_accuracy did not improve from 0.92627\n","Epoch 13/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1541 - accuracy: 0.9331 - val_loss: 0.1155 - val_accuracy: 0.9263\n","\n","Epoch 00013: val_accuracy improved from 0.92627 to 0.92635, saving model to /content/drive/My Drive/models/one_percent_transfer_rgb/model.h5\n","\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n","Epoch 14/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1587 - accuracy: 0.9330 - val_loss: 0.2540 - val_accuracy: 0.9198\n","\n","Epoch 00014: val_accuracy did not improve from 0.92635\n","Epoch 15/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1553 - accuracy: 0.9323 - val_loss: 0.1935 - val_accuracy: 0.9195\n","\n","Epoch 00015: val_accuracy did not improve from 0.92635\n","Epoch 16/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1540 - accuracy: 0.9327 - val_loss: 0.2054 - val_accuracy: 0.9238\n","\n","Epoch 00016: val_accuracy did not improve from 0.92635\n","\n","Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n","Epoch 17/50\n","154/154 [==============================] - 186s 1s/step - loss: 0.1522 - accuracy: 0.9353 - val_loss: 0.2471 - val_accuracy: 0.9195\n","\n","Epoch 00017: val_accuracy did not improve from 0.92635\n","Epoch 18/50\n","154/154 [==============================] - 186s 1s/step - loss: 0.1437 - accuracy: 0.9368 - val_loss: 0.3189 - val_accuracy: 0.9248\n","\n","Epoch 00018: val_accuracy did not improve from 0.92635\n","Epoch 19/50\n","154/154 [==============================] - 186s 1s/step - loss: 0.1383 - accuracy: 0.9390 - val_loss: 0.2598 - val_accuracy: 0.9291\n","\n","Epoch 00019: val_accuracy improved from 0.92635 to 0.92910, saving model to /content/drive/My Drive/models/one_percent_transfer_rgb/model.h5\n","Epoch 20/50\n","154/154 [==============================] - 186s 1s/step - loss: 0.1428 - accuracy: 0.9380 - val_loss: 0.1848 - val_accuracy: 0.9255\n","\n","Epoch 00020: val_accuracy did not improve from 0.92910\n","Epoch 21/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1422 - accuracy: 0.9381 - val_loss: 0.1635 - val_accuracy: 0.9293\n","\n","Epoch 00021: val_accuracy improved from 0.92910 to 0.92929, saving model to /content/drive/My Drive/models/one_percent_transfer_rgb/model.h5\n","Epoch 22/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1478 - accuracy: 0.9366 - val_loss: 0.1964 - val_accuracy: 0.9280\n","\n","Epoch 00022: val_accuracy did not improve from 0.92929\n","Epoch 23/50\n","154/154 [==============================] - 186s 1s/step - loss: 0.1502 - accuracy: 0.9366 - val_loss: 0.2555 - val_accuracy: 0.9203\n","\n","Epoch 00023: val_accuracy did not improve from 0.92929\n","Epoch 24/50\n","154/154 [==============================] - 186s 1s/step - loss: 0.1394 - accuracy: 0.9404 - val_loss: 0.2566 - val_accuracy: 0.8915\n","\n","Epoch 00024: val_accuracy did not improve from 0.92929\n","\n","Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n","Epoch 25/50\n","154/154 [==============================] - 186s 1s/step - loss: 0.1281 - accuracy: 0.9476 - val_loss: 0.3037 - val_accuracy: 0.8880\n","\n","Epoch 00025: val_accuracy did not improve from 0.92929\n","Epoch 26/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1230 - accuracy: 0.9490 - val_loss: 0.1894 - val_accuracy: 0.9098\n","\n","Epoch 00026: val_accuracy did not improve from 0.92929\n","Epoch 27/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1232 - accuracy: 0.9500 - val_loss: 0.2967 - val_accuracy: 0.9281\n","\n","Epoch 00027: val_accuracy did not improve from 0.92929\n","\n","Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n","Epoch 28/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1227 - accuracy: 0.9505 - val_loss: 0.2390 - val_accuracy: 0.9228\n","\n","Epoch 00028: val_accuracy did not improve from 0.92929\n","Epoch 29/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1235 - accuracy: 0.9509 - val_loss: 0.2912 - val_accuracy: 0.9253\n","\n","Epoch 00029: val_accuracy did not improve from 0.92929\n","Epoch 30/50\n","154/154 [==============================] - 187s 1s/step - loss: 0.1215 - accuracy: 0.9509 - val_loss: 0.2974 - val_accuracy: 0.9150\n","\n","Epoch 00030: val_accuracy did not improve from 0.92929\n","\n","Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n","Epoch 00030: early stopping\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Kj7Bk_poV97Q","colab_type":"text"},"source":["#define a list of test image chunks \n"]},{"cell_type":"code","metadata":{"id":"SpryaCe8Al5f","colab_type":"code","outputId":"001add92-7754-4662-e5d0-5e3f163978f2","executionInfo":{"status":"error","timestamp":1589418298778,"user_tz":240,"elapsed":335,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":246}},"source":["\n","test_names=[]\n","\n","with open(test_set_path,\"r\") as f:\n","  test_image_list=[]\n","  for line in f.readlines():\n","    arr=[]\n","    str_array=line.split(\" \")\n","    arr.append(str_array[0]+\" \"+str_array[1])\n","    arr.append(str_array[2]+\" \"+str_array[3][:-1])\n","    test_image_list.append(arr)\n","    test_names.append(line)\n","\n","test_image_list=np.asarray(test_image_list)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-15447bb250df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstr_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest_image_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtest_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"markdown","metadata":{"id":"hZw0KDjFV_yk","colab_type":"text"},"source":["#read test chunks in batches using the list defined above \n"]},{"cell_type":"code","metadata":{"id":"my-pI2L4smKf","colab_type":"code","colab":{}},"source":["\n","import imageio\n","import numpy as np\n","from keras.utils import to_categorical\n","\n","def read_test_image_batch(image_list, batch_size, channel_list):\n","    while True:\n","        l=len(image_list)\n","        num_batch=l//batch_size\n","        if num_batch*batch_size<l:\n","            num_batch+=1\n","        for i in range(num_batch):\n","            batch_set=image_list[batch_size*i:min(batch_size*(i+1),l),:]\n","            batch_set=[batch_set[bs] for bs in range(len(batch_set))]\n","            X=np.array([np.load(line[0][0:]) for line in batch_set])\n","            labels=np.array([np.load(line[1][0:]) for line in batch_set])\n","            y=to_categorical(labels,num_classes)\n","            X = X[:, :, :, channel_list]\n","            yield tuple((X, y))\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AiB6Qm0oWE4m","colab_type":"text"},"source":["#testing model\n"]},{"cell_type":"code","metadata":{"id":"BW4Mjj1a-mY6","colab_type":"code","outputId":"cf2c5274-b247-479a-e354-0bcdca3ec902","executionInfo":{"status":"error","timestamp":1589418316063,"user_tz":240,"elapsed":399,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":246}},"source":["import argparse\n","from keras.models import Model\n","from keras.layers import Activation,Input\n","from scipy.stats import mode\n","import os\n","\n","\n","class_labels=[]\n","\n","\n","for i in os.listdir(model_path):\n","    if(i==\".ipynb_checkpoints\"):\n","        continue\n","    \n","    key=0\n","    #if(key!=0):\n","    #   continue\n","    \n","    print(model_path+i)\n","    my_model=create_model()\n","    my_model.compile(optimizer,loss=loss_function,metrics=metrics)\n","    my_model.load_weights(model_path+i)\n","    \n","    test_data=read_test_image_batch(test_image_list,batch_size,set_info[key])\n","    probs=my_model.predict(test_data,steps=(test_set_size+1)//batch_size)\n","    print(\"\\nprobs\")\n","    print(probs.shape)\n","    class_labels.append(probs.argmax(axis=-1))\n","\n","hist = np.histogram(class_labels[0])\n","class_labels=np.asarray(class_labels)\n","u,indices=np.unique(class_labels,return_inverse=True)\n","final_labels=u[np.argmax(np.apply_along_axis(np.bincount,0,indices.reshape(class_labels.shape),None,np.max(indices)+1),axis=0)]\n","\n","for i in range(test_set_size):\n","    line=test_names[i]\n","    print(line)\n","    str_array=line.split(\" \")\n","    scene_id=str_array[1][18:40]\n","    out_str=str_array[1][40:-4]+\"_output.npy\"\n","    print(out_str)\n","    np.save('/content/drive/My Drive/new_test_outputs_key_0/'+scene_id+out_str,final_labels[i])\n","    "],"execution_count":0,"outputs":[{"output_type":"error","ename":"NotADirectoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-e64513079025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\".ipynb_checkpoints\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/content/drive/My Drive/models/one_percent_transfer/model.h5'"]}]},{"cell_type":"code","metadata":{"id":"JqdvLZbJJ5Qi","colab_type":"code","colab":{}},"source":["\"\"\"\n","This script takes a list of scene ids and creates a file that can be used as input for a segnet model\n","@param string chunk_dir: The abspath base directory where each set of chunks for a scene has its own dir named with its sceneID\n","@param list scene_ids: A list of sceneIDs that exist in the chunk_dir. The chunks of these scenes will be used in the file.\n","@param string out_path: The abspath where the resulting file should be saved.\n","@return int lines_written: the total number of lines (corresponding to a data and label chunk path) in the file.\n","file format:\n","/path/to/scene_chunk.npy,/path/to/scene_chunk_label.npy\n","/path/to/scene_chunk.npy,/path/to/scene_chunk_label.npy\n","/path/to/scene_chunk.npy,/path/to/scene_chunk_label.npy\n","...\n","\"\"\"\n","\n","import os\n","\n","def make_segnet_input_file(chunk_dir, scene_ids, out_path):\n","    existing_scenes = [i for i in os.listdir(chunk_dir) if os.path.isdir(os.path.join(chunk_dir, i))]\n","    # filter out ids that don't exist in the given dir\n","    scene_ids = [i for i in scene_ids if i in existing_scenes]\n","    print(scene_ids)\n","\n","    lines_to_write = []\n","\n","    for i in scene_ids:\n","        scene_dir = os.path.join(chunk_dir, i)\n","        for j in os.listdir(scene_dir):\n","            if j[-9:] != \"label.npy\":\n","                data_path = os.path.join(scene_dir, j)\n","                file_split = os.path.splitext(j)\n","                label_path = os.path.join(scene_dir, file_split[0] + \"_label\" + file_split[1])\n","\n","                lines_to_write.append(\"{},{}\\n\".format(data_path, label_path))\n","\n","    with open(out_path, 'w+') as output:\n","        output.writelines(lines_to_write)\n","\n","    return len(lines_to_write)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"az-Vvf86WQ78","colab_type":"text"},"source":["# Simple script to convert space-delimited chunk-path files to csv for easier file loading.\n","## Old file formats are still available in the metadata directory, but .csv equivalents should be used from now on. This script probably shouldn't been needed again.\n"]},{"cell_type":"code","metadata":{"id":"0disMicfWTZc","colab_type":"code","colab":{}},"source":["import csv\n","metadata_path = \"/content/drive/My Drive/Metadata\"\n","image_files = [i for i in os.listdir(metadata_path) if \".txt\" in i]\n","for i in image_files:\n","    file_name, extension = os.path.splitext(i)\n","    file_path = os.path.join(metadata_path, i)\n","    with open(file_path, 'r') as read_file:\n","        lines = [i[:-1].split(\" \") for i in read_file.readlines() if i]\n","    lines = [[f\"{i[0]} {i[1]}\", f\"{i[2]} {i[3]}\"] for i in lines]\n","\n","    with open(os.path.join(metadata_path, file_name + \".csv\"), 'w') as write_file:\n","        writer = csv.writer(write_file)\n","        writer.writerows(lines)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JM5D3eK4uzSU","colab_type":"code","outputId":"788ac7b4-6482-4580-db06-de5e5a9e1f9b","executionInfo":{"status":"error","timestamp":1589302590191,"user_tz":240,"elapsed":730,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":437}},"source":["metadata_file = [i for i in os.listdir(metadata_path) if \".csv\" in i][0]\n","print(metadata_file)\n","test_img_path = os.path.join(metadata_path, metadata_file)\n","\n","with open(test_img_path) as f:\n","    reader = csv.reader(f)\n","    data = next(reader)\n","\n","print(data[0])\n","print(os.path.isfile(data[0]))\n","\n","test_img = next(data_loader.image_segmentation_generator(data[0], data[1], 1, 2, 512, 512, 512, 512,))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["train.csv\n","/content/drive/My Drive/uncompressed_stacked_chunks/LC80651102015019LGN00/chunk_13_12.npy\n","True\n"],"name":"stdout"},{"output_type":"error","ename":"NotADirectoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-0f7f13cc9ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtest_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_segmentation_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_segmentation/data_utils/data_loader.py\u001b[0m in \u001b[0;36mimage_segmentation_generator\u001b[0;34m(images_path, segs_path, batch_size, n_classes, input_height, input_width, output_height, output_width, do_augment, augmentation_name)\u001b[0m\n\u001b[1;32m    193\u001b[0m                                  augmentation_name=\"aug_all\"):\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mimg_seg_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pairs_from_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_seg_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mzipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_seg_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_segmentation/data_utils/data_loader.py\u001b[0m in \u001b[0;36mget_pairs_from_paths\u001b[0;34m(images_path, segs_path, ignore_non_matching)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0msegmentation_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdir_entry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_entry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_entry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mACCEPTABLE_IMAGE_FORMATS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/content/drive/My Drive/uncompressed_stacked_chunks/LC80651102015019LGN00/chunk_13_12.npy'"]}]}]}
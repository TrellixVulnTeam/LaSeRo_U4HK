{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transfer_segnet.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BZ80yyThVsON","colab_type":"text"},"source":["#Configuration Parameters"]},{"cell_type":"code","metadata":{"id":"AKJ0WYEK39xn","colab_type":"code","outputId":"4eb46100-c4fe-4e62-f0ab-796bd1f0aeda","executionInfo":{"status":"ok","timestamp":1589992191450,"user_tz":240,"elapsed":15605,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":129}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VjH6TLOZq1d4","colab_type":"code","outputId":"bc28a13f-528e-4fc8-c5e6-13a223549fb9","executionInfo":{"status":"ok","timestamp":1589992192392,"user_tz":240,"elapsed":3919,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":329}},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Wed May 20 16:29:52 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qWRDYCBmLAOq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"50b6504f-3c6f-470d-c8ad-c3b68e99d19a","executionInfo":{"status":"ok","timestamp":1589992194473,"user_tz":240,"elapsed":5767,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}}},"source":["import sys\n","import os\n","import argparse\n","import csv\n","\n","import imageio\n","import numpy as np\n","\n","from keras.utils import multi_gpu_model, to_categorical\n","from keras.layers import Activation,Input\n","from keras.models import Model\n","from keras.applications.vgg16 import VGG16\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n","from tensorflow import convert_to_tensor\n","\n","sys.path.append(\"/content/drive/My Drive/tf-keras-SegNet\")\n","from model import segnet"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"n4k2qGbmHZ47","colab_type":"code","colab":{}},"source":["base_dir = \"/content/drive/My Drive/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"anRxznYgHeAV","colab_type":"code","colab":{}},"source":["def create_session_paths(session_name, overwrite=False, base_dir='/content/drive/My Drive/'):\n","    models_dir = os.path.join(base_dir, \"models\")\n","    session_dir = os.path.join(models_dir, session_name)\n","    # Prevent accidental overwriting of previous sessions\n","    try:\n","        os.mkdir(session_dir)\n","    except FileExistsError:\n","        if not overwrite:\n","            print(\"Set overwrite to True if you wish to continue\")\n","            raise FileExistsError\n","        print(\"overwriting session\")\n","\n","    model = os.path.join(session_dir, f\"{session_name}.h5\")\n","    history = os.path.join(session_dir, \"history.json\")\n","    training_log = os.path.join(session_dir, \"logs.csv\")\n","    training_config = os.path.join(session_dir, \"config.json\")\n","    classification_report = os.path.join(session_dir, \"classification_report.txt\")\n","    return {\"model\": model,\n","            \"history\": history,\n","            \"logs\": training_log,\n","            \"config\": training_config,\n","            \"classification_report\": classification_report}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oy2LMxNpHslq","colab_type":"code","colab":{}},"source":["session_name = \"transfer_burjobands_lr_0.001\"\n","session_paths = create_session_paths(session_name, overwrite=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VFdNPyi-H98O","colab_type":"code","outputId":"45dde707-b328-415c-eafb-2acccf6e1532","executionInfo":{"status":"ok","timestamp":1589992208353,"user_tz":240,"elapsed":249,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":109}},"source":["for i in session_paths:\n","    print(session_paths[i])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/models/transfer_burjobands_lr_0.001/transfer_burjobands_lr_0.001.h5\n","/content/drive/My Drive/models/transfer_burjobands_lr_0.001/history.json\n","/content/drive/My Drive/models/transfer_burjobands_lr_0.001/logs.csv\n","/content/drive/My Drive/models/transfer_burjobands_lr_0.001/config.json\n","/content/drive/My Drive/models/transfer_burjobands_lr_0.001/classification_report.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M9idtsPKSejQ","colab_type":"code","colab":{}},"source":["def get_image_list(metadata_file_path):\n","    with open(metadata_file_path, 'r') as f:\n","        return [i for i in csv.reader(f) if i]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"euVM0pt-w0Xs","colab_type":"code","colab":{}},"source":["training_set_path = '/content/drive/My Drive/Metadata/train.csv'\n","val_set_path = '/content/drive/My Drive/Metadata/val.csv'\n","test_set_path = '/content/drive/My Drive/Metadata/test.csv'\n","model_path = session_paths[\"model\"]\n","\n","n_splits = 1\n","bands = [1, 4, 8] # number of bands\n","\n","num_classes = 2\n","image_shape = (512, 512, len(bands))\n","padding = ((0, 0), (0, 0))\n","batch_size = 4\n","epochs = 50\n","learning_rate = 0.001\n","\n","training_set_list = get_image_list(training_set_path)\n","val_set_list = get_image_list(val_set_path)\n","\n","training_set_size = len(training_set_list)\n","val_set_size = len(val_set_list)\n","\n","loss_function = 'categorical_crossentropy'\n","metrics = ['accuracy']\n","callback_metric = \"val_loss\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mCwSixkLDTkN","colab_type":"text"},"source":["# Load segnet and vgg model\n"]},{"cell_type":"code","metadata":{"id":"IR10DGtCLo10","colab_type":"code","outputId":"87d6230d-4ae8-41c9-d2f5-9c339b765243","executionInfo":{"status":"ok","timestamp":1589992247701,"user_tz":240,"elapsed":10973,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":110}},"source":["segnet_model = segnet(image_shape, num_classes)\n","vgg_model = VGG16(include_top=False, weights='imagenet', input_shape=image_shape, classes=num_classes)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Build enceder done..\n","Build decoder done..\n","Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qtQn_aCnaDsB","colab_type":"code","outputId":"5f276f53-05c9-4c36-90b9-ad515fd087dc","executionInfo":{"status":"ok","timestamp":1589992273232,"user_tz":240,"elapsed":264,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["seg_layer_names = [i for i in segnet_model.layers if \"conv\" in i.name]\n","vgg_layer_names = [i for i in vgg_model.layers if \"conv\" in i.name]\n","\n","transferable_layer_names = {}\n","for i in range(len(vgg_layer_names)):\n","    transferable_layer_names[seg_layer_names[i].name] = vgg_layer_names[i]\n","\n","layer_count = 0\n","for i in segnet_model.layers:\n","    try:\n","        i.set_weights(transferable_layer_names[i.name].get_weights())\n","        layer_count += 1\n","    except KeyError:\n","        pass\n","\n","print(layer_count)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["13\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jS01ZdNVDXjJ","colab_type":"text"},"source":["# Transfer weights of matching layers from image-net trained vgg16 to segnet"]},{"cell_type":"markdown","metadata":{"id":"h2NZIihQWBxg","colab_type":"text"},"source":["#Training model \n"]},{"cell_type":"code","metadata":{"id":"55indsfD1s5J","colab_type":"code","colab":{}},"source":["def data_gen(metadata_file_path, bands, batch_size):\n","    image_list = np.asarray(get_image_list(metadata_file_path))\n","    np.random.seed(1)\n","    np.random.shuffle(image_list)\n","\n","    band_normalization_map = []\n","    counter = 0\n","\n","    total_steps = image_list.shape[0] // batch_size\n","    while True:\n","        step_start = counter * batch_size\n","        step_end = step_start + batch_size\n","        images = []\n","        masks = []\n","        for j in range(step_start, step_end):\n","            images.append(np.load(image_list[j, 0])[:,:,bands])\n","            masks.append(np.load(image_list[j, 1]))\n","\n","        y = to_categorical(np.array(masks), 2)\n","        yield np.array(images) / 65535, y.reshape((batch_size, y.shape[1] * y.shape[2], y.shape[3]))\n","\n","        counter +=1\n","\n","        if counter >= total_steps:\n","            counter = 0\n","            np.random.shuffle(image_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SMKEnOm3E8f7","colab_type":"code","outputId":"db1fc3da-d562-401b-bce1-465845c9c19b","executionInfo":{"status":"ok","timestamp":1589914453144,"user_tz":240,"elapsed":9431505,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["train_data = data_gen(training_set_path, bands, batch_size)\n","val_data = data_gen(val_set_path, bands, batch_size)\n","\n","segnet_model.compile(optimizer=Adam(learning_rate=learning_rate), loss=loss_function, metrics=metrics)\n","\n","save_model_path, model_file = os.path.split(session_paths[\"model\"])\n","save_model_file, model_ext = os.path.splitext(model_file)\n","save_model_prefix = os.path.join(save_model_path, save_model_file)\n","\n","accuracy_checkpoint = ModelCheckpoint(f'{save_model_prefix}_val_accuracy.h5',\n","                             monitor='val_accuracy',\n","                             verbose=1,\n","                             save_best_only=True,\n","                             mode='max')\n","\n","loss_checkpoint = ModelCheckpoint(f'{save_model_prefix}_val_loss.h5',\n","                             monitor=callback_metric,\n","                             verbose=1,\n","                             save_best_only=True,\n","                             mode='min')\n","\n","# concatenation of save path is needed here to save model name dynamically by epoch\n","checkpoint = ModelCheckpoint(save_model_prefix + \"_epoch{epoch:02d}.h5\",\n","                             period=5,\n","                             save_weights_only=False,\n","                             save_best_only=False)\n","\n","reduce_lr = ReduceLROnPlateau(monitor=callback_metric,\n","                              factor=0.5,\n","                              patience=3,\n","                              verbose=1,\n","                              mode='min',\n","                              min_lr=0.0001)\n","\n","csv_logger = CSVLogger(session_paths[\"logs\"])\n","\n","early_stopper = EarlyStopping(monitor=callback_metric,\n","                              patience=9,\n","                              verbose=1,\n","                              mode='min')\n","\n","callbacks_list = [accuracy_checkpoint, loss_checkpoint, checkpoint, reduce_lr, csv_logger, early_stopper]\n","\n","try:\n","    model = multi_gpu_model(model)\n","except:\n","    print(\"single GPU in use\")\n","\n","hist = segnet_model.fit(train_data,\n","                        steps_per_epoch=training_set_size // batch_size,\n","                        epochs=epochs,\n","                        validation_data=val_data,\n","                        validation_steps=val_set_size // batch_size,\n","                        verbose=1,\n","                        callbacks=callbacks_list)\n","\n","val_loss = hist.history[\"val_loss\"]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["single GPU in use\n","Epoch 1/50\n","  17/1344 [..............................] - ETA: 2:14:10 - loss: 0.6840 - accuracy: 0.6878"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Kj7Bk_poV97Q","colab_type":"text"},"source":["#define a list of test image chunks \n"]},{"cell_type":"code","metadata":{"id":"JqdvLZbJJ5Qi","colab_type":"code","colab":{}},"source":["\"\"\"\n","This script takes a list of scene ids and creates a file that can be used as input for a segnet model\n","@param string chunk_dir: The abspath base directory where each set of chunks for a scene has its own dir named with its sceneID\n","@param list scene_ids: A list of sceneIDs that exist in the chunk_dir. The chunks of these scenes will be used in the file.\n","@param string out_path: The abspath where the resulting file should be saved.\n","@return int lines_written: the total number of lines (corresponding to a data and label chunk path) in the file.\n","file format:\n","/path/to/scene_chunk.npy,/path/to/scene_chunk_label.npy\n","/path/to/scene_chunk.npy,/path/to/scene_chunk_label.npy\n","/path/to/scene_chunk.npy,/path/to/scene_chunk_label.npy\n","...\n","\"\"\"\n","\n","import os\n","\n","def make_segnet_input_file(chunk_dir, scene_ids, out_path):\n","    existing_scenes = [i for i in os.listdir(chunk_dir) if os.path.isdir(os.path.join(chunk_dir, i))]\n","    # filter out ids that don't exist in the given dir\n","    scene_ids = [i for i in scene_ids if i in existing_scenes]\n","    print(scene_ids)\n","\n","    lines_to_write = []\n","\n","    for i in scene_ids:\n","        scene_dir = os.path.join(chunk_dir, i)\n","        for j in os.listdir(scene_dir):\n","            if j[-9:] != \"label.npy\":\n","                data_path = os.path.join(scene_dir, j)\n","                file_split = os.path.splitext(j)\n","                label_path = os.path.join(scene_dir, file_split[0] + \"_label\" + file_split[1])\n","\n","                lines_to_write.append(\"{},{}\\n\".format(data_path, label_path))\n","\n","    with open(out_path, 'w+') as output:\n","        output.writelines(lines_to_write)\n","\n","    return len(lines_to_write)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"az-Vvf86WQ78","colab_type":"text"},"source":["# Simple script to convert space-delimited chunk-path files to csv for easier file loading.\n","## Old file formats are still available in the metadata directory, but .csv equivalents should be used from now on. This script probably shouldn't been needed again.\n"]},{"cell_type":"code","metadata":{"id":"0disMicfWTZc","colab_type":"code","colab":{}},"source":["import csv\n","metadata_path = \"/content/drive/My Drive/Metadata\"\n","image_files = [i for i in os.listdir(metadata_path) if \".txt\" in i]\n","for i in image_files:\n","    file_name, extension = os.path.splitext(i)\n","    file_path = os.path.join(metadata_path, i)\n","    with open(file_path, 'r') as read_file:\n","        lines = [i[:-1].split(\" \") for i in read_file.readlines() if i]\n","    lines = [[f\"{i[0]} {i[1]}\", f\"{i[2]} {i[3]}\"] for i in lines]\n","\n","    with open(os.path.join(metadata_path, file_name + \".csv\"), 'w') as write_file:\n","        writer = csv.writer(write_file)\n","        writer.writerows(lines)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JM5D3eK4uzSU","colab_type":"code","outputId":"788ac7b4-6482-4580-db06-de5e5a9e1f9b","executionInfo":{"status":"error","timestamp":1589302590191,"user_tz":240,"elapsed":730,"user":{"displayName":"Samuel Elkind","photoUrl":"","userId":"14273351274395620639"}},"colab":{"base_uri":"https://localhost:8080/","height":437}},"source":["metadata_file = [i for i in os.listdir(metadata_path) if \".csv\" in i][0]\n","print(metadata_file)\n","test_img_path = os.path.join(metadata_path, metadata_file)\n","\n","with open(test_img_path) as f:\n","    reader = csv.reader(f)\n","    data = next(reader)\n","\n","print(data[0])\n","print(os.path.isfile(data[0]))\n","\n","test_img = next(data_loader.image_segmentation_generator(data[0], data[1], 1, 2, 512, 512, 512, 512,))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["train.csv\n","/content/drive/My Drive/uncompressed_stacked_chunks/LC80651102015019LGN00/chunk_13_12.npy\n","True\n"],"name":"stdout"},{"output_type":"error","ename":"NotADirectoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-0f7f13cc9ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtest_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_segmentation_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_segmentation/data_utils/data_loader.py\u001b[0m in \u001b[0;36mimage_segmentation_generator\u001b[0;34m(images_path, segs_path, batch_size, n_classes, input_height, input_width, output_height, output_width, do_augment, augmentation_name)\u001b[0m\n\u001b[1;32m    193\u001b[0m                                  augmentation_name=\"aug_all\"):\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mimg_seg_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pairs_from_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_seg_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mzipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_seg_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_segmentation/data_utils/data_loader.py\u001b[0m in \u001b[0;36mget_pairs_from_paths\u001b[0;34m(images_path, segs_path, ignore_non_matching)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0msegmentation_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdir_entry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_entry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_entry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mACCEPTABLE_IMAGE_FORMATS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/content/drive/My Drive/uncompressed_stacked_chunks/LC80651102015019LGN00/chunk_13_12.npy'"]}]}]}